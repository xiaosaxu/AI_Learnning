{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前来说有：1.knn 近邻算法  2.决策树算法（树的生成）3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树 基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  信息量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 度量信息的单位，事件发生的概率越小，表示信息量越大，大家都知道的事件，信息量越小\n",
    "- 两个不相关的事件的信息量为两个事件信息量的和\n",
    "- 表达式为 ：![haha](https://www.zhihu.com/equation?tex=h%28x%29%3D-log_%7B2%7Dp%28x%29+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 熵是在结果出来之前对可能产生的信息量的期望——考虑该随机变量的所有可能取值，即所有可能发生事件所带来的信息量的期望\n",
    "- 表达式为： ![](https://www.zhihu.com/equation?tex=H%28x%29%3D-sum+%28p%28x%29log_%7B2%7Dp%28x%29+%29)\n",
    "- 也就是 [图片链接](https://pic2.zhimg.com/80/v2-a9f081eff039a7e65f51515d4aacb34b_hd.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 条件熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 条件熵的定义是：定义为X给定条件下，Y的条件概率分布的熵对X的数学期望\n",
    "- 因为条件熵中X也是一个变量，意思是在一个变量X的条件下（变量X的每个值都会取），另一个变量Y熵对X的期望"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[条件熵示例](https://pic1.zhimg.com/80/v2-83f2f4b00981806c74e330b2d6f91db5_hd.jpg)  \n",
    "[图片地址](https://pic1.zhimg.com/v2-83f2f4b00981806c74e330b2d6f91db5_r.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设随机变量Y={嫁，不嫁}  \n",
    "我们可以统计出，嫁的个数为6/12 = 1/2  \n",
    "不嫁的个数为6/12 = 1/2  \n",
    "那么Y的熵，根据熵的公式来算，可以得到H（Y） = -1/2log1/2 -1/2log1/2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了引出条件熵，我们现在还有一个变量X，代表长相是帅还是帅  \n",
    "当已知不帅的条件下，满足条件的只有4个数据了，这四个数据中，不嫁的个数为1个，占1/4  \n",
    "嫁的个数为3个，占3/4  \n",
    "那么此时的H（Y|X = 不帅） = -1/4log1/4-3/4log3/4  \n",
    "p(X = 不帅) = 4/12 = 1/3  \n",
    "同理我们可以得到：  \n",
    "当已知帅的条件下，满足条件的有8个数据了，这八个数据中，不嫁的个数为5个，占5/8  \n",
    "嫁的个数为3个，占3/8  \n",
    "那么此时的H（Y|X = 帅） = -5/8log5/8-3/8log3/8  \n",
    "p(X = 帅) = 8/12 = 2/3  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了上面的铺垫之后，我们终于可以计算我们的条件熵了，我们现在需要求：  \n",
    "H（Y|X = 长相）  \n",
    "也就是说，我们想要求出当已知长相的条件下的条件熵。  \n",
    "根据公式我们可以知道，长相可以取帅与不帅俩种  \n",
    "条件熵是另一个变量Y熵对X（条件）的期望。  \n",
    "公式为：  \n",
    "H（Y|X=长相） = p(X =帅)*H（Y|X=帅）+p(X =不帅)*H（Y|X=不帅）  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 我们用另一个变量对原变量分类后，原变量的不确定性就会减小了，因为新增了Y的信息，可以感受一下。不确定程度减少了多少就是信息的增益。__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/26486223  \n",
    "    https://www.zhihu.com/question/22104055 第二个回答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 朴素贝叶斯\n",
    "- 先验与后验概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之所以称为朴素贝叶斯：因为有两个前提：\n",
    "- 假设所有时间都是独立分布的\n",
    "- 将设各个事件的重要性都是一样的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__  近代西方传统中，认为先验指无需经验或先于经验获得的知识。它通常与后验知识相比较，后验意指“在经验之后”，需要经验。这一区分来自于中世纪逻辑所区分的两种论证，从原因到结果的论证称为“先验的”，而从结果到原因的论证称为“后验的”__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 一般来说 先验证概率是 我们通过概率公式计算出来的，后验概率根据实际情况对先验概率进行验证，一样是贝叶斯公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[先验概率与后验概率讲解](https://www.cnblogs.com/yemanxiaozu/p/7680761.html)  \n",
    "[朴素贝叶斯讲解（可结合决策树算法）](http://blog.csdn.net/amds123/article/details/70173402)  \n",
    "[补充（可替换）](http://blog.csdn.net/li8zi8fa/article/details/76176597)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[图片](https://images2017.cnblogs.com/blog/1234526/201710/1234526-20171017121446662-526159147.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考 第一个链接：贝叶斯公式的分母是分嫁与不嫁这个两个条件下的总和，是使用全概率求和的公式计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素贝叶斯之所以朴素有两点：\n",
    "1. 每个元素权重近似认为一样  \n",
    "2. 每个元素之间认为相互独立 产生P（ABCD|E）=P(A|E)* P(B|E) * P(C|E)* P(D|E)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外全概率公式：  \n",
    "P（A）=P(A|B)P(B)+P(A|C)P(C)  \n",
    "即该事件在各个条件下发生的条件概率与该条件发生的概率的乘积 （所有条件之和）  __用于贝叶斯公式分母__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目前来看 这些算法不涉及到 最优化求解问题，直接计算相关出结果，根据结果的大小比较决定类别关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下面介绍逻辑回归算法，涉及到最优化问题求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持向量机 svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVM就是试图把棍放在最佳位置，好让在棍的两边有尽可能大的间隙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[试试](https://leanote.com/api/file/getImage?fileId=5a9d1ca6ab64417d9f00182b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- knernel的作用是生成高维的空间，是的能够找到一个超平面，划分样本点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![地址](https://leanote.com/api/file/getImage?fileId=5a9d10d3ab64417d9f00166a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 这些球叫做 「data」，把棍子 叫做 「classifier」, 最大间隙trick 叫做「optimization」， 拍桌子叫做「kernelling」, 那张纸叫做「hyperplane」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化的目标是 ：在两类区域中各自距离直线最近的点到直线的距离是最大的\n",
    "有两个核心观点：\n",
    "- 1.距离直线最近的点  （称为支持向量）\n",
    "- 2.点到直线的距离最大 （优化条件）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写成公式就是：\n",
    "![](https://leanote.com/api/file/getImage?fileId=5a9de107ab644159cf0004fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 奇异值分解 svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "奇异值往往对应着矩阵中隐含的重要信息，且重要性和奇异值大小正相关。每个矩阵，奇异值越大，重要性越高，占的权重就越大。每个矩阵都可以表示为一系列秩为1的“小矩阵”之和，而奇异值则衡量了这些“小矩阵”对于A的权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我感觉，无论是特征值分解还是奇异值分解，都是为了让人们对矩阵（或者线性变换）的作用有一个直观的认识。这是因为我们拿过来一个矩阵，很多情况下只能看到一堆排列有序的数字，而看不到这些数字背后的真实含义，特征值分解和奇异值分解告诉了我们这些数字背后的真实含义，换句话说，它告诉了我们关于矩阵作用的本质信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 简单地说，奇异值的意义在于一个m*n的矩阵会把n维空间的单位球映射到m维空间的一个椭球(可能是退化的)，而这些奇异值就对应这个椭球的各个半轴长。而奇异向量就恰好是椭球的半轴的方向，以及它们的原像。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 不是所有的矩阵都能对角化（对称矩阵总是可以），而所有矩阵总是可以做奇异值分解的。那么多类型的矩阵，我们居然总是可以从一个统一且简单的视角去看它，我们就会感叹奇异值分解是多么奇妙了！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 一个矩阵越“奇异”，其越少的奇异值蕴含了更多的矩阵信息，矩阵的信息熵越小（这也符合我们的认知，矩阵越“奇异”，其行（或列）向量彼此越线性相关，越能彼此互相解释，矩阵所携带的信息自然也越少）。这些奇异值就是开头我们所谈论的“本质信息”，而从矩阵中也能得到矩阵的“奇异程度”。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "鉴于决策树容易过拟合的缺点，随机森林采用多个决策树的投票机制来改善决策树，我们假设随机森林使用了m棵决策树，那么就需要产生m个一定数量的样本集来训练每一棵树，而最终结果采用Bagging的策略来获得，即多数投票机制得到。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K均值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 马尔可夫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[参考](https://www.zhihu.com/question/29316149)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下面介绍逻辑回归算法，涉及到最优化问题求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性模型实际上是用权重向量W（特征值组成的向量），与样本的向量进行相乘，重合度越高表示越一致，而重合度高的前提的两者的向量的夹角很小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function 表示预测值与真实值之间差距的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
